{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87651260 Applications of Geographic Information System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "## Reference : https://github.com/Gunjan933/twitter-sentiment-analysis\n",
    "\n",
    "## Attawut Nardkulpat, Reseach Officer, Burapha University\n",
    "### attawut@buu.ac.th\n",
    "### 14/08/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:40.638986Z",
     "start_time": "2019-07-12T17:37:38.080731Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --user scipy wordcloud nltk seaborn textblob json nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.249754Z",
     "start_time": "2019-07-12T17:37:40.642457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "nltk.download('wordnet')   # for Lemmatization\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.473952Z",
     "start_time": "2019-07-12T17:37:43.253722Z"
    }
   },
   "outputs": [],
   "source": [
    "total_data = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.485354Z",
     "start_time": "2019-07-12T17:37:43.478411Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('contractions.json', 'r') as f:\n",
    "    contractions_dict = json.load(f)\n",
    "contractions = contractions_dict['contractions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.502718Z",
     "start_time": "2019-07-12T17:37:43.490314Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.535481Z",
     "start_time": "2019-07-12T17:37:43.505212Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking column names into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.550355Z",
     "start_time": "2019-07-12T17:37:43.540924Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet = total_data.columns.values[2]\n",
    "sentiment = total_data.columns.values[1]\n",
    "tweet, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.591020Z",
     "start_time": "2019-07-12T17:37:43.557788Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)  Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Convert every tweets to lower case**\n",
    "* **Remove Twitter username**\n",
    "* **Remove punctuations, numbers and special characters**\n",
    "* **Convert more than 2 letter repetitions to 2 letter ( example (wooooooow --> woow))**\n",
    "* **Remove extra spaces**\n",
    "* **Remove URLs**\n",
    "* **Emoji analysis**\n",
    "* **Handle contractions words**\n",
    "    - **\" can't \" >> \" can not \"**\n",
    "    - **\" won't \" >> \" will not \"**\n",
    "    - **\" should't \" >> \" should not \"**\n",
    "* **Tokenization**\n",
    "* **(Optional) Remove [Stop words](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)**\n",
    "* **(Optional) Text Normalization ([Stemming](https://www.geeksforgeeks.org/python-stemming-words-with-nltk/) / [Lemmatization](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function which handles emoji classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.602426Z",
     "start_time": "2019-07-12T17:37:43.595485Z"
    }
   },
   "outputs": [],
   "source": [
    "def emoji(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :') , :O\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' positiveemoji ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' positiveemoji ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-; , @-)\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' positiveemoji ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:, :-/ , :-|\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' negetiveemoji ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negetiveemoji ', tweet)\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function which will preprocess the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:43.622266Z",
     "start_time": "2019-07-12T17:37:43.605900Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tweet = tweet.lower()                                             # Lowercases the string\n",
    "    tweet = re.sub('@[^\\s]+', '', tweet)                              # Removes usernames\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)   # Remove URLs\n",
    "    tweet = re.sub(r\"\\d+\", \" \", str(tweet))                           # Removes all digits\n",
    "    tweet = re.sub('&quot;',\" \", tweet)                               # Remove (&quot;) \n",
    "    tweet = emoji(tweet)                                              # Replaces Emojis\n",
    "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(tweet))                   # Removes all single characters\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in contractions:\n",
    "            tweet = tweet.replace(word, contractions[word.lower()])   # Replaces contractions\n",
    "    tweet = re.sub(r\"[^\\w\\s]\", \" \", str(tweet))                       # Removes all punctuations\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)                         # Convert more than 2 letter repetitions to 2 letter\n",
    "    tweet = re.sub(r\"\\s+\", \" \", str(tweet))                           # Replaces double spaces with single space    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now make a new column for side by side comparison of new tweets vs old tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.271217Z",
     "start_time": "2019-07-12T17:37:43.626237Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_data['processed_tweet'] = np.vectorize(process_tweet)(total_data[tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T14:50:05.392274Z",
     "start_time": "2019-07-06T14:50:05.387603Z"
    }
   },
   "source": [
    "# Let's compare unprocessed tweets with the processed one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.292567Z",
     "start_time": "2019-07-12T17:37:49.275185Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.307921Z",
     "start_time": "2019-07-12T17:37:49.296514Z"
    }
   },
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n",
    "#total_data['processed_tweet'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "#total_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.322306Z",
     "start_time": "2019-07-12T17:37:49.312386Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_tweet = total_data['processed_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.337185Z",
     "start_time": "2019-07-12T17:37:49.326273Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.359021Z",
     "start_time": "2019-07-12T17:37:49.341154Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.385309Z",
     "start_time": "2019-07-12T17:37:49.365457Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "            \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\n",
    "            \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
    "            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\",\n",
    "            \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
    "            \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
    "            \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
    "            \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\",\n",
    "            \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "            \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\",\n",
    "            \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\",\n",
    "            \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\",\n",
    "            \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
    "            \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
    "            \"can\", \"will\", \"just\", \"should\", \"now\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words, nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:49.572290Z",
     "start_time": "2019-07-12T17:37:49.427953Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiments = ['Positive Sentiment', 'Negetive Sentiment'] \n",
    "slices = [(total_data[sentiment] != 0).sum(), (total_data[sentiment] == 0).sum()] \n",
    "colors = ['g', 'r'] \n",
    "plt.pie(slices, labels = sentiments, colors=colors, startangle=90, shadow = True,\n",
    "        explode = (0, 0.1), radius = 1.5, autopct = '%1.2f%%') \n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Most used positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:54.897533Z",
     "start_time": "2019-07-12T17:37:49.575267Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_words =' '.join([text for text in total_data['processed_tweet'][total_data[sentiment] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21,\n",
    "            max_font_size=110,background_color=\"rgba(255, 255, 255, 0)\"\n",
    "            , mode=\"RGBA\").generate(positive_words)\n",
    "plt.figure(dpi=600)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title(\"Most Used Positive Words\")\n",
    "#plt.savefig('positive_words.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Most used negetive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:37:59.146768Z",
     "start_time": "2019-07-12T17:37:54.901009Z"
    }
   },
   "outputs": [],
   "source": [
    "negetive_words =' '.join([text for text in total_data['processed_tweet'][total_data[sentiment] == 0]])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, \n",
    "            max_font_size=110,background_color=\"rgba(255, 255, 255, 0)\"\n",
    "            , mode=\"RGBA\").generate(negetive_words)\n",
    "plt.figure(dpi=600)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title(\"Most Used Negetive Words\")\n",
    "#plt.savefig('negetive_words.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Feature extraction (vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:07.937872Z",
     "start_time": "2019-07-12T17:37:59.151721Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()    # Unigram and Bigram\n",
    "final_vectorized_data = count_vectorizer.fit_transform(total_data['processed_tweet'])  \n",
    "final_vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting train data to test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:08.178963Z",
     "start_time": "2019-07-12T17:38:07.953247Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, total_data[sentiment],\n",
    "                                                    test_size=0.3, random_state=69)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing splitted dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:08.191298Z",
     "start_time": "2019-07-12T17:38:08.182344Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"X_train_shape : \",X_train.shape)\n",
    "print(\"X_test_shape : \",X_test.shape)\n",
    "print(\"y_train_shape : \",y_train.shape)\n",
    "print(\"y_test_shape : \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Train and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_lr = LogisticRegression().fit(X_train, y_train)\n",
    "predicted_lr = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(dpi=600)\n",
    "mat = confusion_matrix(y_test, predicted_lr)\n",
    "sns.heatmap(mat.T, annot=True, fmt='d', cbar=False)\n",
    "\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "#plt.savefig(confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_lr = accuracy_score(predicted_lr, y_test)\n",
    "print(\"Accuracy with Logistic Regression: \",score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T16:05:19.932965Z",
     "start_time": "2019-07-07T16:05:19.926973Z"
    }
   },
   "source": [
    "### There are some popular classifiers under Naive Bayes\n",
    "* **Bernoulli Naive Bayes**\n",
    "* **Gaussian Naive Bayes classifier**\n",
    "* **Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:08.366853Z",
     "start_time": "2019-07-12T17:38:08.197718Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n",
    "model_naive = MultinomialNB().fit(X_train, y_train) \n",
    "predicted_naive = model_naive.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:10.180262Z",
     "start_time": "2019-07-12T17:38:08.369829Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(dpi=600)\n",
    "mat = confusion_matrix(y_test, predicted_naive)\n",
    "sns.heatmap(mat.T, annot=True, fmt='d', cbar=False)\n",
    "\n",
    "plt.title('Confusion Matrix for Naive Bayes')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "#plt.savefig(confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:10.269290Z",
     "start_time": "2019-07-12T17:38:10.251930Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_naive = accuracy_score(predicted_naive, y_test)\n",
    "print(\"Accuracy with Naive-bayes: \",score_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Precision, Recall, and Accuracy\n",
    "\n",
    "##### Precision, recall, and accuracy are standard metrics used to evaluate the performance of a classifier.\n",
    "\n",
    "* Precision measures how many texts were predicted correctly as belonging to a given category out of all of the texts that were predicted (correctly and incorrectly) as belonging to the category.\n",
    "\n",
    "* Recall measures how many texts were predicted correctly as belonging to a given category out of all the texts that should have been predicted as belonging to the category. We also know that the more data we feed our classifiers with, the better recall will be.\n",
    "\n",
    "* Accuracy measures how many texts were predicted correctly (both as belonging to a category and not belonging to the category) out of all of the texts in the corpus.\n",
    "\n",
    "##### Most frequently, precision and recall are used to measure performance since accuracy alone does not say much about how good or bad a classifier is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model_lr.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(dpi=600)                       # to plot high quality graph\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.ylabel('True Positive Rate', color='g')\n",
    "plt.xlabel('False Positive Rate', color='r')\n",
    "#plt.savefig(\"ROC_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:10.324849Z",
     "start_time": "2019-07-12T17:38:10.277228Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T17:38:12.331660Z",
     "start_time": "2019-07-12T17:38:10.327819Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model_naive.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(dpi=600)                       # to plot high quality graph\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.ylabel('True Positive Rate', color='g')\n",
    "plt.xlabel('False Positive Rate', color='r')\n",
    "#plt.savefig(\"ROC_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "twitter-sentiment-analysis.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}